{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c3ddb9",
   "metadata": {},
   "source": [
    "# Open Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdeec16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2  # Import OpenCV library\n",
    "cap = cv2.VideoCapture(0)  # Create a VideoCapture object to access the default camera (camera index 0)\n",
    "while True:  # Start an infinite loop to continuously capture and display video frames\n",
    "    success, img = cap.read()  # Read a frame from the camera and store it in the variable 'img', 'success' indicates if the frame was read successfully\n",
    "    cv2.imshow(\"Image\", img)  # Display the frame in an OpenCV window with the title \"Image\"\n",
    "    cv2.waitKey(1)  # Wait for a key press event for 1 millisecond, allowing for real-time display of video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "423273b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()  # Release the camera\n",
    "cv2.destroyAllWindows()  # Close all OpenCV windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a56ba8",
   "metadata": {},
   "source": [
    "# Draw LandMarks and Connections in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # Import OpenCV library\n",
    "import mediapipe as mp  # Import Mediapipe library for hand tracking\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Create a VideoCapture object to access the default camera (camera index 0)\n",
    "mpHands = mp.solutions.hands  # Create a Hands object from Mediapipe\n",
    "hands = mpHands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils  # Create a Drawing Utilities object from Mediapipe for drawing landmarks and connections\n",
    "\n",
    "while True:  # Start an infinite loop to continuously capture and process video frames\n",
    "    success, img = cap.read()  # Read a frame from the camera and store it in the variable 'img', 'success' indicates if the frame was read successfully\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert the frame to RGB format as required by Mediapipe\n",
    "    \n",
    "    results = hands.process(imgRGB)  # Process the frame using the Hands object to detect hand landmarks\n",
    "    \n",
    "    if results.multi_hand_landmarks:  # Check if hand landmarks are detected in the frame\n",
    "        for handLms in results.multi_hand_landmarks:  # Loop through each detected hand landmark\n",
    "            mpDraw.draw_landmarks(img, handLms, mpHands.HAND_CONNECTIONS)  # Draw landmarks and connections on the frame using the Drawing Utilities object\n",
    "            \n",
    "    # Put text in the image\n",
    "    cv2.putText(img, \"text\", (10,70), cv2.FONT_HERSHEY_PLAIN, 3, (255,0,255), 3)  # Add text to the frame using OpenCV's putText function\n",
    "    \n",
    "    cv2.imshow(\"Image\", img)  # Display the frame in an OpenCV window with the title \"Image\"\n",
    "    cv2.waitKey(1)  # Wait for a key press event for 1 millisecond, allowing for real-time display of video frames\n",
    "\n",
    "cap.release()  # Release the camera\n",
    "cv2.destroyAllWindows()  # Close all OpenCV windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fe36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()  # Release the camera\n",
    "cv2.destroyAllWindows()  # Close all OpenCV windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a5b10",
   "metadata": {},
   "source": [
    "# Change the color of the landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # Import OpenCV library\n",
    "import mediapipe as mp  # Import Mediapipe library for hand tracking\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Create a VideoCapture object to access the default camera (camera index 0)\n",
    "mpHands = mp.solutions.hands  # Create a Hands object from Mediapipe\n",
    "hands = mpHands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils  # Create a Drawing Utilities object from Mediapipe for drawing landmarks and connections\n",
    "\n",
    "while True:  # Start an infinite loop to continuously capture and process video frames\n",
    "    success, img = cap.read()  # Read a frame from the camera and store it in the variable 'img', 'success' indicates if the frame was read successfully\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert the frame to RGB format as required by Mediapipe\n",
    "    results = hands.process(imgRGB)  # Process the frame using the Hands object to detect hand landmarks\n",
    "    if results.multi_hand_landmarks:  # Check if hand landmarks are detected in the frame\n",
    "        for handLms in results.multi_hand_landmarks:  # Loop through each detected hand landmark\n",
    "            for id, lm in enumerate(handLms.landmark):  # Loop through each landmark point of the hand\n",
    "                h, w, c = img.shape  # Get the height, width, and number of channels of the image\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)  # Convert the normalized landmark coordinates to pixel coordinates\n",
    "                \n",
    "                # Change a particular point (e.g., the first landmark point with id == 0)\n",
    "                if id == 0:\n",
    "                    cv2.circle(img, (cx, cy), 25, (255, 0, 255), cv2.FILLED)  # Draw a filled circle at the landmark point with a specified radius and color\n",
    "            \n",
    "            mpDraw.draw_landmarks(img, handLms, mpHands.HAND_CONNECTIONS)  # Draw landmarks and connections on the frame using the Drawing Utilities object\n",
    "            \n",
    "    # Put text in the image\n",
    "    cv2.putText(img, \"text\", (10, 70), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3)  # Add text to the frame using OpenCV's putText function\n",
    "    \n",
    "    cv2.imshow(\"Image\", img)  # Display the frame in an OpenCV window with the title \"Image\"\n",
    "    cv2.waitKey(1)  # Wait for a key press event for 1 millisecond, allowing for real-time display of video frames\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b63ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()  # Release the camera\n",
    "cv2.destroyAllWindows()  # Close all OpenCV windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02a3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
